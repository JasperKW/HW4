{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import together\n",
    "from time import sleep\n",
    "import re\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dprint(s, debug):\n",
    "    if debug:\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: find your API key here\n",
    "# https://api.together.xyz/settings/api-keys\n",
    "YOUR_API_KEY = '35aa8e364eb9f6f2b4c6e9eb61a7f124be16db13e4d5c7335d9cde39620fe0ab'\n",
    "together.api_key = YOUR_API_KEY\n",
    "\n",
    "def call_together_api(prompt, student_configs, post_processing, model='meta-llama/Llama-2-7b-chat-hf', debug=False):\n",
    "    output = together.Complete.create(\n",
    "    prompt = prompt,\n",
    "    model = model, \n",
    "    **student_configs\n",
    "    )\n",
    "    dprint('*****prompt*****', debug)\n",
    "    dprint(prompt, debug)\n",
    "    dprint('*****result*****', debug)\n",
    "    res = output['output']['choices'][0]['text']\n",
    "    dprint(res, debug)\n",
    "    dprint('*****output*****', debug)\n",
    "    numbers_only = post_processing(res)\n",
    "    dprint(numbers_only, debug)\n",
    "    dprint('=========', debug)\n",
    "    return numbers_only\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Part 1. Zero Shot Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_addition_pairs(lower_bound, upper_bound, rng):\n",
    "    int_a = int(np.ceil(rng.uniform(lower_bound, upper_bound)))\n",
    "    int_b = int(np.ceil(rng.uniform(lower_bound, upper_bound)))\n",
    "    return int_a, int_b\n",
    "\n",
    "def test_range(added_prompt, prompt_configs, rng, n_sample=30, \n",
    "               lower_bound=1, upper_bound=10, fixed_pairs=None, \n",
    "               pre_processing=lambda x:x, post_processing=lambda y:y,\n",
    "               model='meta-llama/Llama-2-7b-chat-hf', debug=False):\n",
    "    int_as = []\n",
    "    int_bs = []\n",
    "    answers = []\n",
    "    model_responses = []\n",
    "    correct = []\n",
    "    prompts = []\n",
    "    iterations = range(n_sample) if fixed_pairs is None else fixed_pairs\n",
    "    for i, v in enumerate(tqdm(iterations)):\n",
    "        if fixed_pairs is None:\n",
    "            int_a, int_b = get_addition_pairs(lower_bound=lower_bound, upper_bound=upper_bound, rng=rng)\n",
    "        else:\n",
    "            int_a, int_b = v\n",
    "        fixed_prompt = f'{int_a}+{int_b}'\n",
    "        fixed_prompt = pre_processing(fixed_prompt)\n",
    "        prefix, suffix = added_prompt\n",
    "        prompt = prefix + fixed_prompt + suffix\n",
    "        model_response = call_together_api(prompt, prompt_configs, post_processing, model=model, debug=debug)\n",
    "        answer = int_a + int_b\n",
    "        int_as.append(int_a)\n",
    "        int_bs.append(int_b)\n",
    "        prompts.append(prompt)\n",
    "        answers.append(answer)\n",
    "        model_responses.append(model_response)\n",
    "        correct.append((answer == model_response))\n",
    "        sleep(1) # pause to not trigger DDoS defense\n",
    "    df = pd.DataFrame({'int_a': int_as, 'int_b': int_bs, 'prompt': prompts, 'answer': answers, 'response': model_responses, 'correct': correct})\n",
    "    print(df)\n",
    "    mae = mean_absolute_error(df['answer'], df['response'])\n",
    "    acc = df.correct.sum()/len(df)\n",
    "    prompt_length = len(prefix) + len(suffix)\n",
    "    res = acc * 1/prompt_length * (1-mae/(5*10**6))\n",
    "    return {'res': res, 'acc': acc, 'mae': mae, 'prompt_length': prompt_length}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",  #LLaMa-2-7B\n",
    "    \"meta-llama/Llama-2-13b-chat-hf\", #LLaMa-2-13B\n",
    "    \"meta-llama/Llama-2-70b-hf\" #LLaMa-2-70B\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Zero-shot single-digit addition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-2-7b-chat-hf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/var/folders/hp/nv9fn33j7vjfj9dnxyv_30dw0000gn/T/ipykernel_55735/3950626566.py:7: DeprecationWarning: Call to deprecated function create.\n",
      "  output = together.Complete.create(\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'output'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     27\u001b[0m rng \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mdefault_rng(seed)\n\u001b[0;32m---> 28\u001b[0m res \u001b[38;5;241m=\u001b[39m test_range(added_prompt\u001b[38;5;241m=\u001b[39madded_prompt, prompt_configs\u001b[38;5;241m=\u001b[39mprompt_config, rng\u001b[38;5;241m=\u001b[39mrng, n_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, lower_bound\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, upper_bound\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, fixed_pairs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pre_processing\u001b[38;5;241m=\u001b[39myour_pre_processing, post_processing\u001b[38;5;241m=\u001b[39myour_post_processing, model\u001b[38;5;241m=\u001b[39mmodel, debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(res)\n",
      "Cell \u001b[0;32mIn[31], line 26\u001b[0m, in \u001b[0;36mtest_range\u001b[0;34m(added_prompt, prompt_configs, rng, n_sample, lower_bound, upper_bound, fixed_pairs, pre_processing, post_processing, model, debug)\u001b[0m\n\u001b[1;32m     24\u001b[0m prefix, suffix \u001b[38;5;241m=\u001b[39m added_prompt\n\u001b[1;32m     25\u001b[0m prompt \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m fixed_prompt \u001b[38;5;241m+\u001b[39m suffix\n\u001b[0;32m---> 26\u001b[0m model_response \u001b[38;5;241m=\u001b[39m call_together_api(prompt, prompt_configs, post_processing, model\u001b[38;5;241m=\u001b[39mmodel, debug\u001b[38;5;241m=\u001b[39mdebug)\n\u001b[1;32m     27\u001b[0m answer \u001b[38;5;241m=\u001b[39m int_a \u001b[38;5;241m+\u001b[39m int_b\n\u001b[1;32m     28\u001b[0m int_as\u001b[38;5;241m.\u001b[39mappend(int_a)\n",
      "Cell \u001b[0;32mIn[30], line 15\u001b[0m, in \u001b[0;36mcall_together_api\u001b[0;34m(prompt, student_configs, post_processing, model, debug)\u001b[0m\n\u001b[1;32m     13\u001b[0m dprint(prompt, debug)\n\u001b[1;32m     14\u001b[0m dprint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*****result*****\u001b[39m\u001b[38;5;124m'\u001b[39m, debug)\n\u001b[0;32m---> 15\u001b[0m res \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     16\u001b[0m dprint(res, debug)\n\u001b[1;32m     17\u001b[0m dprint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*****output*****\u001b[39m\u001b[38;5;124m'\u001b[39m, debug)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'output'"
     ]
    }
   ],
   "source": [
    "added_prompt = ('Question: What is ', '?\\nAnswer: ') # Question: What is a+b?\\nAnswer:\n",
    "prompt_config = {'max_tokens': 2,\n",
    "                'temperature': 0.7,\n",
    "                'top_k': 50,\n",
    "                'top_p': 0.6,\n",
    "                'repetition_penalty': 1,\n",
    "                'stop': []}\n",
    "\n",
    "# input_string: 'a+b'\n",
    "def your_pre_processing(input_string):\n",
    "    return input_string\n",
    "\n",
    "# output_string: \n",
    "# depending on your prompt, it might look like 'output: number'\n",
    "def your_post_processing(output_string):\n",
    "    # using regular expression to find the first consecutive digits in the returned string\n",
    "    only_digits = re.sub(r\"\\D\", \"\", output_string)\n",
    "    try:\n",
    "        res = int(only_digits)\n",
    "    except:\n",
    "        res = 0\n",
    "    return res\n",
    "\n",
    "model = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "print(model)\n",
    "seed = 0\n",
    "rng = np.random.default_rng(seed)\n",
    "res = test_range(added_prompt=added_prompt, prompt_configs=prompt_config, rng=rng, n_sample=10, lower_bound=1, upper_bound=10, fixed_pairs=None, pre_processing=your_pre_processing, post_processing=your_post_processing, model=model, debug=False)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Zero-shot 7-digit addition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/var/folders/hp/nv9fn33j7vjfj9dnxyv_30dw0000gn/T/ipykernel_55735/1481791231.py:7: DeprecationWarning: Call to deprecated function create.\n",
      "  output = together.Complete.create(\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'output'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m prompt_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m      3\u001b[0m rng \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mdefault_rng(seed)\n\u001b[0;32m----> 4\u001b[0m res \u001b[38;5;241m=\u001b[39m test_range(added_prompt\u001b[38;5;241m=\u001b[39madded_prompt, prompt_configs\u001b[38;5;241m=\u001b[39mprompt_config, rng\u001b[38;5;241m=\u001b[39mrng, n_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, lower_bound\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000000\u001b[39m, upper_bound\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9999999\u001b[39m, fixed_pairs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pre_processing\u001b[38;5;241m=\u001b[39myour_pre_processing, post_processing\u001b[38;5;241m=\u001b[39myour_post_processing, model\u001b[38;5;241m=\u001b[39mmodel, debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(res)\n",
      "Cell \u001b[0;32mIn[16], line 26\u001b[0m, in \u001b[0;36mtest_range\u001b[0;34m(added_prompt, prompt_configs, rng, n_sample, lower_bound, upper_bound, fixed_pairs, pre_processing, post_processing, model, debug)\u001b[0m\n\u001b[1;32m     24\u001b[0m prefix, suffix \u001b[38;5;241m=\u001b[39m added_prompt\n\u001b[1;32m     25\u001b[0m prompt \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m fixed_prompt \u001b[38;5;241m+\u001b[39m suffix\n\u001b[0;32m---> 26\u001b[0m model_response \u001b[38;5;241m=\u001b[39m call_together_api(prompt, prompt_configs, post_processing, model\u001b[38;5;241m=\u001b[39mmodel, debug\u001b[38;5;241m=\u001b[39mdebug)\n\u001b[1;32m     27\u001b[0m answer \u001b[38;5;241m=\u001b[39m int_a \u001b[38;5;241m+\u001b[39m int_b\n\u001b[1;32m     28\u001b[0m int_as\u001b[38;5;241m.\u001b[39mappend(int_a)\n",
      "Cell \u001b[0;32mIn[14], line 15\u001b[0m, in \u001b[0;36mcall_together_api\u001b[0;34m(prompt, student_configs, post_processing, model, debug)\u001b[0m\n\u001b[1;32m     13\u001b[0m dprint(prompt, debug)\n\u001b[1;32m     14\u001b[0m dprint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*****result*****\u001b[39m\u001b[38;5;124m'\u001b[39m, debug)\n\u001b[0;32m---> 15\u001b[0m res \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     16\u001b[0m dprint(res, debug)\n\u001b[1;32m     17\u001b[0m dprint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*****output*****\u001b[39m\u001b[38;5;124m'\u001b[39m, debug)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'output'"
     ]
    }
   ],
   "source": [
    "sleep(1) # wait a little bit to prevent api call error\n",
    "prompt_config['max_tokens'] = 8\n",
    "rng = np.random.default_rng(seed)\n",
    "res = test_range(added_prompt=added_prompt, prompt_configs=prompt_config, rng=rng, n_sample=10, lower_bound=1000000, upper_bound=9999999, fixed_pairs=None, pre_processing=your_pre_processing, post_processing=your_post_processing, model=model, debug=False)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1a.** In your opinion, what are some factors that cause language model performance to deteriorate from 1 digit to 7 digits?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1b**. Play around with the config parameters ('max_tokens','temperature','top_k','top_p','repetition_penalty') in together.ai's [web UI](https://api.together.xyz/playground/language/togethercomputer/llama-2-7b). \n",
    "* What does each parameter represent?\n",
    "* How does increasing each parameter change the generation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1c**. Do 7-digit addition with 70B parameter llama model. \n",
    "* How does the performance change?\n",
    "* What are some factors that cause this change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep(1) # wait a little bit to prevent api call error\n",
    "rng = np.random.default_rng(seed)\n",
    "res = test_range(added_prompt=added_prompt, prompt_configs=prompt_config, rng=rng, n_sample=10, lower_bound=1000000, upper_bound=9999999, fixed_pairs=None, pre_processing=your_pre_processing, post_processing=your_post_processing, model='meta-llama/Llama-2-70b-hf', debug=False)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1d.** Here we're giving our language model the prior that the sum of two 7-digit numbers must have a maximum of 8 digits. (by setting max_token=8). What if we remove this prior by increasing the max_token to 20? \n",
    "* Does the model still perform well?\n",
    "* What are some reasons why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep(1) # wait a little bit to prevent api call error\n",
    "added_prompt = ('Question: What is ', '?\\nAnswer: ') # Question: What is a+b?\\nAnswer:\n",
    "prompt_config = {'max_tokens': 20,\n",
    "                'temperature': 0.7,\n",
    "                'top_k': 50,\n",
    "                'top_p': 0.6,\n",
    "                'repetition_penalty': 1,\n",
    "                'stop': []}\n",
    "\n",
    "# input_string: 'a+b'\n",
    "def your_pre_processing(input_string):\n",
    "    return input_string\n",
    "\n",
    "def your_post_processing(output_string):\n",
    "    first_line = output_string.splitlines()[0]\n",
    "    only_digits = re.sub(r\"\\D\", \"\", first_line)\n",
    "    try:\n",
    "        res = int(only_digits)\n",
    "    except:\n",
    "        res = 0\n",
    "    return res\n",
    "\n",
    "\n",
    "model = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "print(model)\n",
    "seed = 0\n",
    "rng = np.random.default_rng(seed)\n",
    "res = test_range(added_prompt=added_prompt, prompt_configs=prompt_config, rng=rng, n_sample=10, lower_bound=1000000, upper_bound=9999999, fixed_pairs=None, pre_processing=your_pre_processing, post_processing=your_post_processing, model=model, debug=False)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. In Context Learning\n",
    "\n",
    "We will try to improve the performance of 7-digit addition via in-context learning.\n",
    "For cost-control purposes (you only have $25 free credits), we will use [llama-2-7b](https://api.together.xyz/playground/language/togethercomputer/llama-2-7b). Below is a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep(1) # wait a little bit to prevent api call error\n",
    "added_prompt = ('Question: What is 3+7?\\nAnswer: 10\\n Question: What is ', '?\\nAnswer: ') # Question: What is a+b?\\nAnswer:\n",
    "prompt_config = {'max_tokens': 8,\n",
    "                'temperature': 0.7,\n",
    "                'top_k': 50,\n",
    "                'top_p': 0.6,\n",
    "                'repetition_penalty': 1,\n",
    "                'stop': []}\n",
    "rng = np.random.default_rng(seed)\n",
    "res = test_range(added_prompt=added_prompt, prompt_configs=prompt_config, rng=rng, n_sample=10, lower_bound=1000000, upper_bound=9999999, fixed_pairs=None, pre_processing=your_pre_processing, post_processing=your_post_processing, model='meta-llama/Llama-2-7b-chat-hf', debug=False)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2a**.\n",
    "* How does the performance change with the baseline in-context learning prompt? (compare with \"Example: Zero-shot 7-digit addition\" in Q1)\n",
    "* What are some factors that cause this change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will remove the prior on output length and re-evaluate the performance of our baseline one-shot learning prompt. We need to modify our post processing function to extract the answer from the output sequence. In this case, it is the number in the first line that starts with \"Answer: \"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2b**.\n",
    "* How does the performance change when we relax the output length constraint? (compare with Q2a)\n",
    "* What are some factors that cause this change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep(1) # wait a little bit to prevent api call error\n",
    "\n",
    "prompt_config['max_tokens'] = 50 # changed from 8, assuming we don't know the output length\n",
    "                \n",
    "rng = np.random.default_rng(seed)\n",
    "res = test_range(added_prompt=added_prompt, prompt_configs=prompt_config, rng=rng, n_sample=10, lower_bound=1000000, upper_bound=9999999, fixed_pairs=None, pre_processing=your_pre_processing, post_processing=your_post_processing, model='meta-llama/Llama-2-7b-chat-hf', debug=False)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2c.** Let's change our one-shot learning example to something more \"in-distribution\". Previously we were using 1-digit addition as an example. Let's change it to 7-digit addition (1234567+1234567=2469134). \n",
    "* Evaluate the performance with max_tokens = 8.\n",
    "* Evaluate the performance with max_tokens = 50.\n",
    "* How does the performance change from 1-digit example to 7-digit example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep(1) # wait a little bit to prevent api call error\n",
    "prompt_config['max_tokens'] = 8 \n",
    "added_prompt = ('Question: What is 1234567+1234567?\\nAnswer: 2469134\\nQuestion: What is ', '?\\nAnswer: ') # Question: What is a+b?\\nAnswer:\n",
    "test_range(added_prompt=added_prompt, prompt_configs=prompt_config, rng=rng, n_sample=10, lower_bound=1000000, upper_bound=9999999, fixed_pairs=None, pre_processing=your_pre_processing, post_processing=your_post_processing, model='meta-llama/Llama-2-7b-chat-hf', debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep(1) # wait a little bit to prevent api call error\n",
    "prompt_config['max_tokens'] = 50 \n",
    "test_range(added_prompt=added_prompt, prompt_configs=prompt_config, rng=rng, n_sample=10, lower_bound=1000000, upper_bound=9999999, fixed_pairs=None, pre_processing=your_pre_processing, post_processing=your_post_processing, model='meta-llama/Llama-2-7b-chat-hf', debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2d.** Let's look at a specific example with large absolute error. \n",
    "* Run the cell at least 5 times. Does the error change each time? Why?\n",
    "* Can you think of a prompt to reduce the error?\n",
    "* Why do you think it would work?\n",
    "* Does it work in practice? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_range(added_prompt=added_prompt, prompt_configs=prompt_config, rng=rng, fixed_pairs=[(9090909,1010101)], pre_processing=your_pre_processing, post_processing=your_post_processing, model='meta-llama/Llama-2-7b-chat-hf', debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Prompt-a-thon (autograder & leaderboard)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compete with your classmates to see who's best at teach llama to add 7-digit numbers reliably! Submit your ```submission.py``` to enter the leader board!\n",
    "\n",
    "Note: while you can use prompt.txt for debugging and local testing, for the final autograder submission, please use a string (not a file), because autograder cannot find prompt.txt in the testing environment. Sorry about the inconvenience!\n",
    "\n",
    "What you can change:\n",
    "* your_api_key\n",
    "* your_prompt\n",
    "* your_config\n",
    "* your_pre_processing\n",
    "* your_post_processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
